《算法导论》May 11 25-36  
如果存在常量a、b，使得对于足够大的任意n，f(n)都能在a*g(n)和b*g(n)之间，则称g(n)是f(n)的**渐进紧确界**，    
或者记作f(n)=θ(g(n))。  
例如，f(n)为一个二阶多项式，那么一定存在a和b在g(n)=n<sup>2</sup> 时，g是f的渐进紧确界。直观点理解就是，  
n足够大时，低次项被忽略，二阶项是单调递增的。  
当只有**渐进上界**时，用O表示上述渐进关系，f(n)=O(g(n))。Ω记号表示**渐进下界**。二者都比θ更弱（无需紧确）。  
插入排序的时介于Ω(n)~O(n<sup>2</sup>)，考虑最差情况，一定存在输入使其有Ω(n<sup>2</sup>)，所以其时间复杂度为O(n<sup>2</sup>)。  
o记号不要求渐进紧确（对任意的c都有f(n)<=cg(n)），比O弱；类似地，ω比Ω弱。

《算法导论》May 13 37-42  
**递归式**可以方便地表示分治算法的时间，T(n)=aT(n/b)+f(n)，第一项中a是分解后要处理的次数，b是子问题规模。  
a和b可能不相等。第二项是分解与合并的耗时。  
给一段时间内的股票交易价格，求买卖股票一次的时机，使收益最大。将价格转为股票涨幅数组，该问题可转化为    
求解涨幅数组的最大子数组问题。  
用分治思想考虑如何求解数组的最大子数组，考虑子数组相对于数组中点的位置，只有三种可能：
1. 完全位于左边
2. 完全位于右边
3. 跨过中点，这种情况下，只需在两边各遍历一次就能找到最大sum和左右位置，线性时间复杂度。

然后递归二分数组，比较左、右和跨中点的最大子数组和，最大的对应左右位置就是结果。  
递归式T(n)=aT(n/b)+θ(n)，所以复杂度是θ(nlgn)。  
实际上最优解的时间复杂度是θ(n)，用滑动窗口解。

《算法导论》May 14 43-50  
两个n*n矩阵的点乘，按照定义可得时间复杂度为O(n<sup>3</sup>)，而这并非最优解，Strassen分治的算法更优。  
为了引出此算法，先考虑将两矩阵分别4等分的分治，因为子问题规模变为原来的1/2，而要进行8次递归调用，  
故T(n)=8T(n/2)+f(n<sup>2</sup>)（n>1），其中8可以看作递归树的分叉数。  
Strassen算法很不直观，其核心思想是用n/2的八个矩阵两两加减共10个作为临时矩阵，再7次乘法得多项式，  
最终结果可由多项式加减得到。相当于减少了分叉，从8减成7，时间复杂度为O(n<sup>log2(7)</sup>)  
递归式求解方法，先猜测形式，再用数学归纳法求解其中的常数。

《算法导论》May 15 51-64  
**递归树**可用来生成好的猜测，每个节点表示单一子问题的代价，对每层求和后再求和，得到递归调用总代价。  
**主定理：** a>=1, b>1，T(n) = aT(n/b) + f(n)，T(n)的渐进界如下：
1. 若对某个常数ε>0有f(n) = θ(n<sup>logb(a)-ε</sup>)，则T(n) = θ(n<sup>logb(a)</sup>)。
2. 若f(n) = θ(n<sup>logb(a)</sup>)，则T(n) = θ(n<sup>logb(a)</sup>lg(n))。
3. 若对某个常数ε>0有f(n)=θ(n<sup>logb(a)+ε</sup>)，且在n足够大时对小于1的常数c有af(n/b)< = cf(n)，则T(n) = θ(f()n)。 

直观理解，还是舍去增长率低的项。但是三种情况并未覆盖所有，适用范围有限。  
**主方法**就是利用套主定理直接得到结果，例如T(n) = 9T(n/3) + n，因为n = n<sup>log3(9)-1</sup>，属情况1 => T(n) = θ(n<sup>2</sup>)。  
再例如Strassen方法，其递归式为T(n) = 7T(n/2) + θ(n<sup>2</sup>)，仍然符合情况1。


《算法导论》May 16 65-73  
雇佣问题：面试1~n共n个候选人，如果比当前员工好就辞掉后者，雇佣前者，辞旧雇新花销比单独面试花销大。  
```
for i in candidates  
    interview i  
    if i better than the current
        current = i
        hire i
```
显然面试花销跟候选人水平的序列有关，最差情况是序列是递增的，最好是递减的。因为顺序未知，所以可通过   
**概率分析**来计算平均情况运行时间。更一般地，如果算法的行为由输入决定，且由随机数产生的数值决定，则称  
这个算法是**随机的**，随机算法的运行时间称期望运行时间。  
用指示器随机变量解上述问题，X<sub>i</sub>表示第i个应聘者被雇佣，则整个面试雇佣过程：  
X = ∑X<sub>i</sub>，对该多项式取期望得E[X] = ∑E[X<sub>i</sub>]（因为X是线性的），  
第i个应聘者比0~i-1优秀的概率是1/i（因为应聘者能力的随机性），故  
E[X] = ∑1/i = lnn + O(1)

《算法导论》May 17 74-90
1. 排序对象经常是记录，不是单纯的排序关键字，额外包含了**卫星数据**。  
2. 一个排序算法如果输入数组中只有常数个元素需要存储在排序过程的存储外，则称其为**原址的**。  
3. **堆**是一个数组，可以被看成拉成一条线的二叉树。按照父子节点的相对大小关系，分为大根堆和小根堆。
4. 大根堆的heapify过程，输入是数组和元素位置i，比较i元素和子节点，较小则降，过程时间为O(lgn)。
5. **建堆**过程从有子节点的最后一个元素开始往前遍历做heapify，时间复杂度是O(nlgn)。
6. 利用建好的堆循环：移掉顶部，最后一个移到顶，对顶做heapify。则移掉的元素有序，时间复杂度也是  
O(nlgn)。
   
《算法导论》May 18 91-106
1. **优先队列**是堆的常见应用，最大优先队列支持插入，返回/弹出最大值和更新（增加）key值操作。
2. heap-insert过程——增加元素，更新key值，重复往上跟比它小的值交换，时间复杂度是O(lgn)。
3. **快速排序**跟归并排序一样用到分治思想，取数组第一个元素为枢值，先partition——将比它小的放左边  
比它大的放右边；再对两边递归调用快速排序。partition过程可以原址进行。时间复杂度O(nlgn)。
4. 快排最差时间复杂度为 O(n<sup>2</sup>)，比如数组已有序，每次选枢值划分的数组的平衡性越好，性能越好。
5. 为了避免最差情况，可以随机化枢值的选取，期望运行时间是O(nlgn)。

《算法导论》May 20 107-118
1. **比较排序**指的是根据元素间比较的排序，前面集中都是，它们都最坏情况下都要经过Ω(nlgn)次比较。证明：  
  一个长度为n的数组，排列组合有n！种顺序，将比较过程看成决策树，则每种可能都会出现在叶子节点，  
  树的高度可以确定。现在考虑一棵高度为h、有l个可达叶节点的决策树，则n!<=l，而二叉树节点数目不可能  
  多于2<sup>h</sup>，两边取对数 => h >= lg(n!) = Ω(nlgn)。下面几个都是线性时间复杂度的排序。
2. **计数排序**针对的是输入元素都不大的情况，为每个元素计数（存放到临时数组），从高到低读该数组元素值  
  每读一位置就把相应个数的下标值放到添加到第三个数组，它就是结果。
3. **基数排序**跟计数排序差不多，要求元素数值在某个不大的范围内。将序列每个元素逐个取某一位的值作排序  
  依据，将元素分放到表示位值的数组里，从低位到高位循环该过程。最后从9~0依次读取元素，完成排序。  
4. **桶排序**适用于元素在[0,1)均匀分布的序列，按照元素大小分别放入对应的桶中，则每个桶里放了一个链表，  
   插入排序链表，然后从小到大遍历桶以及其中的元素，完成排序。


《算法导论》May 21 108-126
1. 求序列**中位数**（或**第i小问题**），可以使用快排用到的分区法，当pivot左边有i-1个数时，pivot就是结果；  
  当pivot左边有k个数时，若k<i-1则从右边序列找第i-k小，否则还从左边找第i小（递归），每次丢掉一部分。  
  该算法最差时间复杂度是O(n<sup>2</sup>)，期望时间复杂度O(n)。
2. 该问题的最坏情况为线性时间的算法：
   1. 划分序列为n/5组，每组5个元素。
   2. 插排法寻找每一组的中位数。
   3. 递归该算法找到2中的n/5个中位数的中位数x。
   4. 用x划分序列，后面步骤跟上面相似，丢掉一部分。  

   最坏情况比1的还优，本质是选了更中间的数，让分区更高效了。

《算法导论》May 22 127-141  
1. 关于**基本数据结构**的知识，涵盖栈、队列、链表和树。
2. 链表中有时用哑对象，主要是为了简化代码，空间多用一点，效率不能得到提升。
3. 在不支持显式指针和对象数据类型的语言中，可用多数组或单数组实现它们。比如多数组：

|Addr|pre|key|next
|---|---|---|---|
|1|||
|2|5|4|3
|3|2|1|/
|4|||
|5|7|16|2
|6|||
|7|/|9|5

可以表示{9, 16, 4, 1}按顺序构成的双向链表，实际上也可表示更复杂的对象。  
4. 左孩子右兄弟表示法适用于分支数任意的有根树。

《算法导论》May 23 142-148  
1. **散列表**可以看作数组的扩展，由key得到索引直接取value，索引的确定有下面方法：
   1. **直接寻址**，直接用key作为数组索引，散列表够大能包含这些key。
   2. **散列法**，用散列函数寻址，key是自变量，一般比较随机。
2. 不同key代入散列函数可能得到相同值，这时就需要处理冲突，方法有：
   1. **链接法**，将有相同索引的元素放到链表中。
   2. **开放寻址法**，所有元素都存在散列表里。
3. 装载因子α=n/m，n是能存的元素数上限，m是表的槽位数。可证明链接法一次查表失败的平均时间：θ(1+α)
4. 好的散列函数的特点是，每个key都尽可能等概率散列到任意一个槽位，常见的有乘法、除法。

《算法导论》May 25 148-160
1. 散列表的散列方法中，开放寻址法跟链接法比，优势在于不存链表故少了指针，潜在地增大了槽位数。
2. 用开放寻址法插入关键字k时，若遇到槽位已占，则需要重复**探查**，方法有：
   1. 线性探查：h(k,i = (h'(k)+1)) mod m, 其中i=0,1,...,m-1。容易出现一次群集问题，探查耗时越来越长。
   2. 二次探查：h(k,i) = (h'(k)+ai+bi<sup>2</sup>) mod m, 其中i=0,1,...,m-1。较上面也存在程度较轻的二次群集。
   3. 双重散列：h(k,i) = (h1(k) + i h2(k)) mod m。探查的序列会因为h2(k)变化，随机性最强。
3. 对于双重散列法，为了查找整个散列表，h2(k)必须与m互素。一种方法就是取m2的幂，并让h2总为奇数。  
  还可以取m为素数，h2返回比m小的正整数。随机性强的根因是h1和h2可精心设计，使得对冲突的k1和k2  
  在下次探查时的值不同，查找序列更加随机。
4. **完全散列**是指最坏情况下也能用O(1)完成访存，当关键字是静态（∈特定集合）时就可以设计出这种函数。

《算法导论》May 27 161-172
1. **二叉搜索树**性质：对于任意节点x，x左子树节点的key都不大于x.key，x右子树的key都不小于x.key。
2. 二叉搜索树的基本**查询**操作（maximum，minimum，successor和predecessor）时间复杂度都是O(lgn)。
   1. successor的查找：若有右子树，则是右子树的最左孩子节点；否则是往上追溯到的祖先节点第一次作为  
     左孩子出现的父节点。
  ```python
    successor(x):
    if x.right != null
      return tree_minimum(x.right)
    y = x.p
    while y != null && x == y.right
      x = y
      y = y.p
    return y
  ```
   2. predecessor的查找：若有左子树，则是左子树的最右孩子节点；否则是自己作为右孩子的父节点。
3. 往二叉搜索树**插入**节点时，也是遵循待插入值较大则往右、小则往左的规则（树为空的边界没考虑）：
  ```python
  tree_insert(T, z):
  p = null
  x = T.root
  while x != null
    p = x
    if z.key <= x.key 
      x = x.left
    else x = x.right
  if z.key <= y.key
    y.left = z
  else y.right = z
  ```
4. **删除**节点较麻烦，需要查找和删除，然后动态调整树的结构（移植）使其仍是二叉搜索树。当删除的节点  
有左孩子且右孩子也有左孩子时，对右孩子做右旋——左子为父并取代删除的节点。


《算法导论》May 29 174-177
1. **红黑树**是一种考虑了平衡性的搜索树，基本操作最坏复杂度是O(log n)，有以下性质：
   1. 每个节点是红或黑色的
   2. 根节点是黑色的
   3. 每个叶节点（nil）是黑色的
   4. 红色节点的左右孩子都是黑色的
   5. 每个节点到其所有后代叶节点的简单路径上，均包含相同数目的黑色节点。
2. 由上述性质可以证明，有n个内部节点（不含nil）的红黑树，高度最多为2lg(n+1)。
3. 树可以**旋转**，通常用来保证插入、删除了节点后的树的平衡性，左旋伪代码如下：
  ```
  left_rotate(T, x):
    y = x.right
    x.right = y.left  // x接管y的左孩子
    if y.left != nil
      y.left.p = x
    
    y.p = x.p    // y取代x的位置
    if x.p == nil
      T.root = y
    elseif x == x.p.left
      x.p.left = y
    else x.p.right = y
    
    y.left = x  // 父子颠倒
    x.p = y
  ```
  ``` 
    |                |
    x                y
 /   \             /  \ 
 a    y     =》   x    c  
      / \        / \       
      b  c      a   b        
 ```

《算法导论》May 31 177-180
1. 红黑树的**插入**：第一步，跟普通搜索树的插入一样找位置，然后将颜色设为红。  
  第二步：为使树还是红黑树，需要进行调整，有几种情况（用z表示待插入节点）：
   1. z的叔节点y是红色：将z.p和y描黑，将z.p.p描红。将z.p.p作为新z循环该过程。
   2. y是黑色且z是右孩子：左旋一下转变为情况3，按3处理.
   3. y是黑色且z是左孩子：将子树y右旋，并把新的子树根节点描黑，俩子节点描红。
2. 只有情况1的处理次数跟树高有关，时间复杂度O(log n)。其他情况都是O(1)。


《算法导论》Jun 1 181-192
1. 红黑树的**删除**节点的操作分两步，一删二调整，跟插入操作类似，着重说一下二者的区别：
   1. 始终维持y为重树中删除的或移至树内的结点，z子结点<2时y指向z，否则y指向z的后继，移至树中，  
   用x记录原来的位置。
   2. y被移动后，用z的颜色作为y的新颜色，并暂存原颜色给后面调整以维持红黑树性质用。
   3. y原为黑色才可能引起树性质变化，需要进一步调整颜色。
2. 调整也有几种情况：
   1. x的兄弟结点w是红色的：旋转x.p和w并改变二者的颜色，转为下面情况。下面几种都是w为黑的情况。
   2. w的两个子结点是红色：从x和w去掉一重黑色，在x.p上补偿一重额外的黑色。以x.p为新的x循环判断。
   3. w的孩子是红色右孩子黑色：交换w和左孩子的颜色，右旋一下变成情况4.
   4. w的右孩子红色：修改某些颜色再左旋x.p，去掉额外的黑色。


《算法导论》June 3 193-198
1. 当简单的数据结构不能满足需求时，有时可以通过**扩张**数据结构来实现。
2. **顺序统计树**（前面说的找第i小的数）即通过扩张红黑树实现的，给结点增加size属性，简单二分查找即可。
3. 另外给定一个元素要确定它的**秩**（即序号i）同样快捷——它及其祖先结点左孩子(size+1)之和。
4. 扩张的数据结构的一个重要操作是维护其额外属性。顺序统计树插入时，第一阶段需沿线路径size加一，  
  第二阶段旋转，最多影响两个结点的size值。整个操作时间O(log n)。


《算法导论》June 4 198-202  
1. **区间树**（interval tree），是一种对动态集合进行维护的红黑树，每个结点包含**区间**属性int，支持的操作：
   1. interval-insert(T, x)：插入x到T。
   2. interval-delete(T, x)：从T中删除x。
   3. interval-search(T, i)：返回**一个**与i有重叠的结点。
2.  **重叠**有四种情况，如下台所示，精简为一个条件 `i.low <= x.high && i.high >= x.low`。

      ```
      i   --   ----  ---    ---
      x  ----   --    ---  ---
         情况1   2     3     4
      ```
3. 在红黑树的基础上，结点还需要属性：int和max，int为区间含俩端点值，**max**作为key是子树最大端点值。
4. interval-search(T, i)的实现：
      ```
      interval-search(T, i):
        x = T.root
        while x.left != T.nil and i does not overlap x.int
          if x.left!= T.nil and x.left.max >= i.low
            x = x.left      // i左端在x左孩子右端的左边，往左走
          else x = x.right  // 否则，左边不可能有跟i有重叠的区间了，往右走
      ```

《算法导论》June 5 203-210
1. **动态规划**，通常用于求解最优化问题，通过逐步选择，把问题变成成形式相同的子问题，关键技术就是保存  
  子问题的解，避免重复计算。即也是它跟分治算法的区别，即动态规划选择过程中的子问题有相交。
2. 例，一段长为n的钢条，可以切成长度为i的段，不同长度的钢条有不同价格p<sub>i</sub>，如何切割可使卖钱最多。  
分析：用P(n)表示卖的钱数，当第一次的长度是i，则P(n, i) = p<sub>i</sub> + P(n-i)，使P最大的方案即最优方案。  
将上面思路用伪代码写出来，时间复杂度O(2<sup>n</sup>)：  
      ```
      P(p, n)
      if n == 0
        return 0
      q = -∞
      for i = 1 to n
        q = max(q, p[i] + P(p, n-i))
      ```
   
   这是一个自顶向下的算法，时间复杂度高的原因是重复计算，比如方案一的前两次切掉1、2的长度，方案二  
   先切3的长度，二者剩余子问题的结果都是P(p, n-3)。优化办法很简单，就是记下所有子问题的P的函数值：
   ``` 
   P(p, n, r)  // 数组r记录子问题函数值
   if r[n] >= 0
     return r[n]
   if n == 0
     q = 0
   else  
     q= -∞
     for i = 1 to n
       q = max (q, p[i] + P(p, n-i, r))
   r[n] = q
   return q
   ```
   
    用迭代代替递归，改成自底向上，可以进一步优化时间按复杂度的常数项：  
   ```
   bottom-up(p, n)
     for j = 1 to n
       q = -∞
       for i = 1 to j
         q = max(q, p[i]+r[j-i])
       r[j] = q
     return r[n]
   ```

《算法导论》June 7 211-215
1. **矩阵链乘法**问题：给定n个矩阵的链<A1, A2, ..., An>，矩阵Ai的规模是p<sub>i-1</sub> × p<sub>i</sub>（0<i<=n）以使其乘法相容，  
  求完全括号化方案（即乘法的先后顺序），使得计算乘积所需标量乘法次数最少。
2. 为了弄清楚问题，可以假设有A、B、C、D四个矩阵，它们链乘有如下5种结合方式：  
   (A(B(CD))))  
   (A((BC)D)))  
   ((AB)(CD)))  
   (((AB)C)D))  
   ((A(BC))D))  
3. 两个矩阵的点乘算法：
      ```
      maxtrix_multiply(A, B):
        let C be a A.rows × B.columns maxtix
        for i = 1 to A.rows
          for j = 1 to B.columns
            c = 0
            for k = 1 to A.columns
              c += + A[i,k]*B[k,j]  // 1次标量乘法
            C[i,j] = c
        return C
      ```
   设A和B的规模分别是p×q和q×r，则可以看出二者标量乘法规模是p*r*q。如果是三个矩阵链乘，规模分别为  
   2×3, 3×10, 10×2  
  若先结合前俩，则计算的代价是2 × 3 × 10 + 2 × 2 × 10 = 100.  
  若先结合后俩，则计算的代价是3 × 2 × 10 + 2 × 2 × 3 = 72（更优）。   
  问题的本质就是，为矩阵链乘找到一种标量乘法最少的结合方式。
4. 如何计算括号化方案的数量？可以将完全括号化的矩阵乘积看作两个完全括号化的部分积的相乘，部分积  
    作为因数分别是k和(n-k)个矩阵的乘积，思路有了——使俩子两个子问题有最优解，代价m的递归式如下：
  m[i,j] = m[i,k] + m[k+1,j] + p<sub>i</sub>p<sub>i-k</sub>p<sub>j</sub> (当i=j时，没有标量乘法，代价为0)。
5. 于是递归算法就有了：  
    ``` 
    matrix_chain_order(p):
      n = p.len - 1
      // s记录括号位置相关信息，用于构造最优解
      let m[1..n, 1..n] =and s[1..n-1, 2..n] be new tables
      for i = 1 to n
        m[i,j] = 0
      for l = 2 to n
        for i = 1 to n-l+1
          j = i+l-1
          m[i, j] = ∞
          for k = i to j-1
            q = m[i,k] + m[k+1,j] + p[i-1]p[k]p[j]
            if q < m[i,j]
              m[i,j] = q
              s[i,j] = k
      return m and s     
    ```
   这个算法的时间复杂度是O(n<sup>3</sup>)，空间复杂度是O(n<sup>2</sup>)，而暴力尝试的时间复杂度为指数级。
6. 最优解的构造：  
   ```
   print_optimal_patterns(s, i, j):
     if i == j
       print "A"i
     else 
       print "("
       print_optimal_patterns(s, i, s[i,j])
       print_optimal_patterns(s, s[i,j]+1, j)
       print ")"  
   ```

《算法导论》June 8 215-222
1. 什么样的问题适合用动态规划解？同时具备两个要素：
   1. 最优子结构：问题的最优解包含其子问题的最优解。要确定这一点，可以套一个通用模式：  
     做出一个选择，这次选择会产生哪些子问题，确定子问题的解就是其本身的最优解。
   2. 子问题重叠：两种选择不同，但它们产生的子问题会有重叠。
2. 刻画子问题空间的经验是，尽可能简单，必要时才扩展。比如前面的切钢条问题，问题空间：对每个i值，  
  长度为i的钢条的最优切割，这个是合适的。与之相对地，我们企图限制矩阵链乘法问题的子问题空间，  
  除非我们能保证k恒等于j-1，否则就会发现得到两个形如A1A2A3...Ak和Ak+1Ak+2Ak+3...Aj的子问题，  
  后者跟原问题没啥差别。因此对矩阵链乘法问题，必须允许子问题在两端都可以变化。
3. 动态规划算法有两种方式——自顶向下+备忘，自底向上。后者效率高，不过前者更接近自然智慧（递归）  
  当不必完全求解所有子问题时，前者更适用。
 

《算法导论》June 9 222-225
1. **最长公共子序列**LCS问题。对于A和B两个序列，长度分别为m和n，则其LCS（设为C，长度为k）可知：
   1. 若A和B的末尾元素相等，则前缀C<sub>k-1</sub>是A<sub>m-1</sub>与B<sub>m-1</sub>的LCS。
   2. 否则，当C、A的末尾元素不同，C是A<sub>m-1</sub>跟B<sub>n</sub>的LCS
   3. 否则，当C、B的末尾不同，C是A<sub>m</sub>跟B<sub>n-1</sub>的LCS。
2. 由此可以写出自底向上的动态规划算法：
    ```
    lcs_length(X, Y):
      m = X.length
      n = Y.length
      // c存放X、Y不同前缀的LCS，b记录公共元素的位置
      let b[1..m, 1..n] and c[0..m, 0..n] be new tables
      for i = 1 to m
        c[i, 0] = 0
      for j = 0 to n
        c[0, j] = 0
      for i = 1 to m
        for j = 1 to n
          // 为了方便，x,y的下标从1开始 
          if x[i] == y[i]  // 情况1.i
            c[i, j] = c[i-1, j-1] + 1
            b[i, j] = '↖'
          else if c[i-1, j] >= c[i, j-1]  // 情况1.ii
            c[i, j] = c[i-1, j]
            b[i, j] = '↑'
          else c[i, j] = c[i, j-1] // 情况1.iii
            b[i, j] = '←'
      return c and b
    ```
3. 根据b可以构造LCS，遇到b[i,j]=↖说明X[i,j]是公共序列的一个元素，以此类推，递归逆序打印出即可。
4. 改进LCS的构造：省去二维数组b，用一个list存储公共元素值即可。不过c决定了空间复杂度仍是O(mn)。

《算法导论》June 10 226-236
1. **最优二叉搜索树**问题：用一组关键字构造二叉搜索树，查找每个关键字有特定的概率，如何构造二叉搜索树  
  才能使得代价（访问节点数）最小。其中被搜索的关键字可能不存在，称为伪关键字。
2. 将问题形式化，关键字（排序后）用K=<k1, k2, ..., kn>表示，则伪关键字D=<d0, d1, ..., dn>。另外这些值  
  被搜索到的概率分别用p<sub>i</sub>（i∈[1,n]）和q<sub>i</sub>（i∈[0,n]）表示。
3. 考虑一个最优二叉搜索树T，它必须包含连续关键字k<sub>i</sub>, ..., k<sub>j</sub>，而且其叶节点必然是伪关键字d<sub>i-1</sub>, ..., d<sub>j</sub>，可知  
  若T'是T的一个子树，则它必然是最优二叉搜索树（反证法易证）。故可用子问题最优解构造原问题最优解。  
  子问题域：求解包含关键字k<sub>i</sub>, ..., k<sub>j</sub>的最优二叉搜索子树。定义e[i,j]为期望代价。原问题需要e[1,n]最小。
   1. 当j=i-1时，只有伪关键字构成子树，e[i, i-1]=q<sub>i-1</sub>。
   2. 当j>=i时，选r位置的关键字作为根节点，则e[i, j] = p<sub>r</sub> + (e[i,r-1] + w(i, r-1)) + (e[r+1, j] + w(r+1, j))，其中  
   w(i, j)表示k<sub>i</sub>, ..., k<sub>j</sub>概率之和，故上式简化为：  
   **e[i] = min<sub>i<=r<=j</sub>{ e[i, r-1] + e[r+1, j] + w(i, j) }**
4. 根据递归公式可以写出自底向上的动态规划算法：
      ```
      optimal_bst(p, q, n):
        let e[1..n+1, 0...n], w[1..n+1, 0..n], and root[1..n, 1..n] be new tables
        for i = 1 to n+1
          e[i, j-1] = q[i-1]
          w[i, i-1] = q[i-1]
        for l=1 to n
          for i = n to n-l+1
            j = i+l-1
            e[i,j] = ∞
            w[i,j] = w[i, i-1] + p[j] + q[i]
            for r = i to j
              t = e[i, r-1] + e[r+1,j] + w[i,j]
              if t < e[i,j]
                e[i,j] = t
                root[i,j] = r
        return e and root
      ```

《算法导论》June 13 237-241
1. **贪心算法**：总是做出当下最优的选择，有时候可以因此获得全局最优，比动态规划简单。
2. 比如安排活动问题：有n个活动的集合S={a1, a2, ... ,an}，a<sub>i</sub>的开始时间为s<sub>i</sub>，结束时间为f<sub>i</sub>，选择安排哪些  
  活动才能让活动数最多，且满足时间上没有交叠？
3. 贪心策略可以是，总选择最早结束的活动。故可以对活动按照结束时间排序，然后遍历选择：  
    ```
    recursive_activeity_selector(s, f):
      n = s.length
      A = {a[1]}
      k = 1
      for i = 2 to n
        if s[i] >= f[k]
          A = A ∪ {a[i]}
          k = i
      return A
    ```


《算法导论》June 14 242-249
1. 通过对活动安排问题的探讨，我们知道可以在动归基础上优化子问题，以获得单一子问题。然后要证明选择  
  后原问题存在最优解，最后证明剩余子问题满足：最优解与贪心选择组合即可得到原问题的最优解。其中，  
  贪心选择的性质和最优子结构是关键要素。
2. 最优子结构是说，一个问题的最优解包含其子问题的最优解。比如若S<sub>ij</sub>包含活动a<sub>k</sub>，则它必然也包含子问题  
   S<sub>ik</sub>和S<sub>kj</sub>的最优解。更直接一点只需证明：组合子问题最优解和贪心选择即可得到子问题（数学归纳法）。
3. 0-1背包问题：小偷要用能装重量W的包装n件物品，后者重量和价格已知，装哪些物品才能得到最高价值？  
  0-1表示的是，对任意一件物品的选/不选。一个贪心算法是选单价（价格/重量）最高的，但不一定会得到  
  最优解（对于分数背包问题可行），因为这样的贪心可能会让能装的物品的重量下降。
4. **赫夫曼编码**：一种变长二进制编码（也是前缀码），在已知字符出现频率的情况下，可以使占用空间最小。  
  它也是用贪心的思想，将出现频率最低的两个字符作为二叉树的子节点，二者频率之和作为新节点的频率；  
  重复上面过程，最后可以获得赫夫曼树，每个字符的编码都在叶子节点，其编码就是根到它的左/右选择。


《算法导论》June 17 258-260
1. **摊还分析**，用数据结构中一个操作序列中所执行的所有操作的平均时间来评价操作代价。
2. 一种方法是**聚合分析**。比如对空栈操作的问题，它本来有两种基本操作push和pop，现在加一个multipop，    
  求n个由它仨组成的序列的操作代价。很容易看出，一次操作最坏代价是O(n)，故有n次操作的上界是O(n²)。  
  但是它并非确界，因为一个元素入栈，我们最多只能将其弹出一次。所以两种弹出操作最多跟push次数一样  
多，所以O(n)才是更准确的答案，所以摊还代价是O(1)。
3. 另一个例子，二进制计数递增。k位二进制数用数组A存放每一位，下标大的存高位。可以用这种方式让A  
  递增：由低到高遍历A每个元素，若为1则反转为0，直到遇到0就将其赋值为1。n次递增代价是多少？  
  这还是单次递增最坏情况是O(n)的问题，但n次递增代价累计起来（不大于一个1/2为比值的等比数列），  
  发现它是收敛到2n的，所以每次代价仍是O(1)。

《算法导论》June 19 261-264
1. **核算法**是另一种摊还分析法，基本思想：计算每次操作的代价时可以多算，多算的部分即信用，信用可以  
  用来抵消后续操作的等量代价。需要保证累积信用永远不小于0，以使得总摊还代价总是实际代价的上界。
2. 用这个方法解push、pop、multipop问题就更直观了，一次push操作原本代价为1，记为2，多出的1是信用。  
  其他操作的代价都记为0，信用不会透支，因为栈中元素不会少于0个. 同样得出，O(n)是操作代价的上界。
3. 对于数组表示的二进制计数器的自增问题，同样记置位操作代价为2，其中信用为1，也能得出同样的结论。
4. **势能法**是第三种摊还分析法，核心思想：将预付代价表示为势能，不同于信用，势能跟整个数据结构关联。  
  对一个初始数据结构D<sub>0</sub>执行n个操作，c<sub>i</sub>为第i个操作的实际代价，c'<sub>i</sub>为摊还代价，将势函数Φ定义为：  
   c'<sub>i</sub> = c<sub>i</sub> + Φ(D<sub>i</sub>) - Φ(D<sub>i-1</sub>)    （公式1）  
   n次操作累加，中间D<sub>i</sub>一加一减都被消掉了，通常将Φ(D<sub>0</sub>)定义为0，可得：  
   ∑c' = ∑c + Φ(D<sub>n</sub>)   
  为了让总摊还代价是总实际代价的上界，即∑c' >= ∑c，只需Φ(D<sub>n</sub>) >= Φ(D<sub>0</sub>)，充分条件是Φ(D<sub>i</sub>) >= Φ(D<sub>0</sub>)。
5. 将势能法用到栈操作上，定义栈中元素数量为势能函数，push的势能差为1，实际代价也为1，根据公式1  
  可知一次push的摊还代价为2. 类似地求得弹出操作的摊还代价为0，同样有结论：一系列操作的代价O(n)。


《算法导论》June 20 265-274
1. **动态表**：当插入的过多，使空间不够用时，就扩张表空间；反之，元素删除到一定程度也减小表空间。
2. 表扩张：假设装载因子α永远不低于0.5，表满后的一次插入触发扩张。假设每次插入代价为1，表扩张代价  
  等于当前表的大小。根据聚合分析，Σc<sub>i</sub> = n + Σ2<sup>j</sup>，其中j ∈ [0, lgn]，可以证明n个插入的总代价（包括扩表）  
  为3n，故摊还代价为O(1)。
3. 现在同时考虑表的收缩，不能想当然地把装载因子低于0.5作为收缩条件，否则可能会面临O(n)的均摊代价：  
  当n是2的幂且前一半是插入操作、后一半删除时。因此将0.25设为缩表的条件。假设势函数在α>=0.5时为  
  2T.num - T.size，在α<0.5时为T.size/2 - T.num。根据势函数可以根据定义分析得到，引起缩表的删除操作  
  的摊还代价为2，其他删除操作代价为1.因此可以得到动态表的一些列n个操作的均摊代价为O(1)。

《算法导论》June 21 277-281
1. **B树**是为磁盘等存储设备设计的平衡搜索树。它类似于红黑树，但分支因子可以很大，能大幅减少I/O操作。
  B树具有以下性质：
   1. 每个结点包含三个属性：x.n为当前存储在结点x中的关键字个数，x.key<sub>i</sub>是关键字，x.leaf是否叶子。
   2. 每个内部结点还包含x.n+1个指向其孩子的指针x.c<sub>i</sub>。
   3. x.key<sub>i</sub>分割了x.c.key。
   4. 每个叶结点有相同的深度h。  
   5. 每结点所包含的关键字的个数有上下界，用整数t(>1)表示这些界：除了根的结点至少有t-1个关键字。  
    每个节点至多包含2t-1个关键字（满的）。
2. B树的关键字和卫星数据是放在一起的，实际的磁盘页可能是这些数据，也可能是指向它们的指针。B+树  
  跟B树比，卫星数据放到了叶子结点内部节点只存放关键字和孩子指针，因此可以使分支因子最大化。
3. 在典型的B树的应用场景中，数据量大而无法一次加载到内存，故一次只读若干页面到内存，反之写亦然。  
  通常一个B树算法的运行时间由读写次数决定，所以讲B树节点设计得跟一个磁盘页一样大。

《算法导论》June 25 281-289
1. 搜索B树：从根结点开始，拿目标k与结点的n个key从左到右比较大小，遇到第一个大于k的key，位置为i，  
 则将第i个子结点作为新节点递归。该过程结束的条件是找到等于k的key或Nil。
2. 向B树插入一个key：当插入后key数达到(2t-1)时，就将该结点分裂，中间key提到父结点，两边各分得  
   (t-1)个key作为新的两个孩子结点。如果父结点因此也达到(2t-1)个key，则同样分裂，向上递归。
3. 从B树中删除key，有以下情况：
   1. 待删关键字在叶结点，直接删除。
   2. k在内部节点x时，有以下情况：
      1. 若钱于k的子结点y至少包含t个关键字，则找出k在以y为根的子树中的前驱k'，递归地删除k'，在x中  
       用k'代替k。
      2. 若y有少于t的关键字，则检查结点x中后于k的子结点z，若z至少有t个关键字则找出k在以z为根的  
        子树中的后继k'，递归地删除k'，并在x中用k'代替k。
      3. 若y和z都只含有t-1个关键字，则合并z进y，释放z，递归地从y中删除k。
   3. 若k不在内部结点x中，则先确定包含k的子树的根x.c<sub>i</sub>，若它只有t-1个关键字，则执行下面a或b步骤来  
     保证降至一个至少包含t个关键字的结点，再对x的合适的子结点进行递归。
      1. 若x.c<sub>i</sub>有t-1个关键字，相邻兄弟至少有t个关键字，则将x中的某一关键字降至x.c<sub>i</sub>，将x.c<sub>i</sub>的相邻  
      左或右兄弟的一个关键字升至x，该兄弟中相应的孩子指针移到x.c<sub>i</sub>中。
      2. 若x.c<sub>i</sub>和x.c<sub>i</sub>的所有相邻兄弟都只包含t-1个关键字，则跟一个兄弟合并，即x的一个关键字会成为  
       新合并结点的的中间关键字。
      

《算法导论》June 30 290-292  
1. 斐波那契堆，是一系列具有最小堆序的有根树的集合，每棵树都遵循最小堆性质。任意结点的子结点以  
  双向链表的形式构成一个环，称孩子链表。根节点也相连形成环形双向链表，称为根链表。结点有指向父  
  和孩子的指针各一个。
2. 应用环形链表在斐波那契堆有两个优点：可以在O(1)时间内从一个环形双向链表的任何位置插入或删除  
  一个节点。可以在O(1)时间将两个这种链表链接成一个环形双向链表。
3. 除了上述属性，每个节点还有mark表示自从上一次成为孩子结点后是否失去过孩子，有degree表示该结点  
  的孩子数。堆还有个属性min指向最小根节点。
4. 斐波那契堆的势函数如下：  
   Φ(H) = t(H) + 2m(H)  
  两项分别是跟链表中结点数（树的数目）和已标记结点的数目。

《算法导论》July 2 293-297
1. 斐波那契堆的插入与合并都是O(1)摊还代价的操作，只需将根链表插入结点或将两个链表合并。  
2. 最麻烦的操作是删除最小结点EXTRACT-MIN，它包含以下步骤：
   1. 删除最小结点，将其孩子链表加入根链表，更新最小值的指针。
   2. 合并根链表，使得每个根有不同的度数，下面是具体步骤：
      1. 初始化一个表存放以根链表每个节点为根的树的度。
      2. 如果一个树的度跟已存在的相等，就将两个树合并到一起。合并时将key更小的作为父节点。
      3. 对应地更新表中的度，递归进行上一步和这一步，直至没有度冲突。
   3. 更新最小结点指针。
3. 其摊还代价是O(log n)，用势函数法：操作之前的势为t(H) + 2m(H)，因为最多有D(n) + 1个根留下且过程中  
没有结点被标记，所以操作之后的代价为D(n) + 1 + 2m(H)，故摊还代价最多为O(D(n)) = O(log n)。

《算法导论》July 3 299-300
1. 斐波那契堆的关键字减值（将x结点的关键字减为k，违反最小堆序的情况下）过程：
   1. cut操作：将x从父结点y剪掉加入根节点链表，x节点的mark和父指针清空。
   2. 然后cascade-cut操作y节点：在y的父z不空时，将y.mark置位，若其已置位则cascade-cut操作z。
   3. 最后判断x的关键字是否小于H.min，小于则更新H.min。
2. 下面考虑一下减值的时间复杂度：
   1. 因为对x的操作是常数时间，而cascade-cut操作递归调用c次，则实际代价为O(c)。
   2. 势的变化包括切掉结点和清除标志位，最多有m(H) - c + 2个被标记的节点，可得势的变化最多为：  
      (t(H) -+c) + 2*(m(H) - c + 2) - (t(H) - 2*m(H)) = 4 - c
   3. 两个结果相加，可知摊还代价是O(1).
   4. 另外势函数的定义在m(H)前加了系数2，是因为1要支付切断和标记的清除，1补偿节点y变根增加的势。
3. 删除一个结点x的操作就是减值操作（减为-∞）跟删除最小值的结合。
   

《算法导论》July 6 301-305
1. 为了证明删除斐波那契堆中删除最小节点的摊余时间复杂度为O(log n)，可以从堆的势函数着手逐步分析。  
   斐波那契堆维护一组树，势函数Φ=堆中树的数量+标记节点数量的两倍。删除最小节点时，执行以下步骤：
   1. 从根列表中删除最小节点：该操作将堆中的树数量减少一棵。因此，Φ减一。
   2. 将最小节点的子节点与根列表合并：最小节点的每个子节点都成为根列表中的一棵新树。由于每棵树  
   都对势函数有贡献，因此Φ随着子树数量的增加而增加。
   3. 合并根列表中的树：将相同度数的树合并，直到没有两棵树具有相同的度数。在此过程中树的数量  
      减少，但Φ不变。
   4. 如有必要，更新最小节点，对势函数没有影响。
      
2. 现在，考虑最坏的情况，即堆中的所有节点都被标记。在这种情况下，标记的节点数最多为堆中的节点数，  
  即n。因此，势函数可以以2n为界。删除最小节点的实际时间复杂度在最坏情况下为O(log n)。这是因为  
  在合并过程中，根列表中的树数量最多可以是log n。其他步骤具有恒定的时间复杂度。  
 通过运算后的Φ减去运算前的Φ，可得删除斐波那契堆中最小节点的摊余时间复杂度为O(log n)。

《算法导论》July 7 306-308
1. 动态集合的集几种存储方法：
   1. **直接寻址**：用u位的数组A[0..u-1]存储一个值来自全域{0,1,2,...,u-1}的动态集合。u[x]=1表示x存在。  
   这种结构插入、删除的时间复杂度都是O(1)，但找前驱、后继和最值需要θ(u)的时间。
   2. **叠加的二叉树结构**：在上述A结构基础上叠加一个二叉树，A的每相邻的两个元素（位）值的逻辑或  
    的结果作为父节点的键。以此类推往上堆叠。找前驱、后继就成了循着1向上再向右、左找1的过程。  
   于是，前面线性时间复杂度的操作都被优化成O(lg u)。另外，插入和删除需要付出额外代价更新父的  
   键，原本O(0)时间复杂度的操作恶化成O(lg u)。
   3. **叠加一个高度恒定的树**：考虑全域的大小u=2<sup>2k</sup>，其中k为整数。则√u是整数，作为所叠加树的高度。  
   每个内部节点的键是√u个子的键的或。A[i√u, (i+1)√u - 1]被称为A的第i个簇，插入变成O(1)的操作，  
  因为高度固定了。查找最值和前驱后继时间复杂度变差为O(√u)，看起来高度恒定的树没什么性能  
   提升，对，这只是van Emde Boas的铺垫，√u高度的树是其关键。

《算法导论》July 9 309-313
1. 前面说度为√u的树是为vEB树作铺垫，后者操作时间是O(lg lgu)，为达到这个目标，可以考虑一个递归式：  
  **T(u) = T(√u)  + O(1)**  
  令m=lg u，代入上式有T(2<sup>m</sup>) = T(2<sup>m/2</sup>) + O(1)  
  再令S(m) = T(2<sup>m</sup>)，得S(m) = S(m/2) = O(1)，它的解是 S(m) = O(lg m)，所以T(u) = O(lg lg u)。
2. 上面递归式满足需要，它对应的数据结构是：第k层的全域大小是第k-1层大小的平方根。即递归结构各全域  
  大小序列为u，u<sup>1/2</sup>，u<sup>1/4</sup>，……考虑存储全域所需位数，则每层需要前一层的一半。
3. 根据上面推导，递归定义原型vEB树，每个proto-vEB(u)结构都包含属性u代表全域大小，另有以下特征：
   1. 若u=2，则它是基础大小，只包含一个两位的数组A[0..1]。  
   2. 否则u=2<sup>2<sup>k</sup></sup>，proto-vEB(u)还有summary属性，指向proto-vEB(√u)结构；数组cluster[1..√u-1]属性，每个  
  元素的指针都指向一个proto-vEB(√u)。  
    ![](../../../Library/Containers/com.tencent.xinWeChat/Data/Library/Application Support/com.tencent.xinWeChat/2.0b4.0.9/cc0f17aee32b331b7b27e058482f2373/Message/MessageTemp/9e20f478899dc29eb19741386f9343c8/Image/101688913712_.pic.jpg)
4. 上述结构的member(x)操作（判断一个值是否在集合中）和min(x)都需要时间O(lg lgu)。

《算法导论》July 10 314-323
1. van Emde Boas树是proto-van Emde Boas树的改进，增加了两个属性max和min，另外对u值限制放松为  
  2的自然数n次幂，当n不是整数时，(lg u)/2的上平方根为summary数组大小。
2. 另外min不存在于任何一个簇中，max相反。有了这两个属性，一些操作的时间性能就能得到优化：
   1. maximum和minimum操作都可以直接返回。
   2. successor操作可以避免一个用于判断值x的后继是否位于high(x)中的递归调用，因为当且仅当x严格  
   小于x簇的max时，x的后继才位于x簇中。
   3. 在insert和delete操作中帮助判空。
   4. 更新max和min即可完成对空树的插入，反过来delete也是如此。
3. 用递归式刻画时间复杂度：T(u) <= T(上√u) + O(1)，而m>=2时，m/2<=2m/3，故可得  
  T(2<sup>m</sup>) = T(2<sup>2m/3</sup>) + O(1)，解得T(u) = O(lg lgu)。  
  一棵vEB树的总空间需求是O(u)，因此不应把它用于只需少量操作的场景。


《算法导论》July 11 324-328
1. **不相交集合**数据结构是一组不相交的动态集合，每个集合有个成员作为**代表**标识该集合，我们不关心代表  
  是谁，但要求二次查询没改变的集合的代表时，结果一致。它支持以下操作：
   1. make-set(x)，建立新集合，不相交的性质暗示x不会出现在其他集合中。
   2. union(x, y)，合并两个集合为一个，并且把原有集合删除。实际上常变通为合并一个集合到另一个中。
   3. find-set(x)，返回包含x的集合的代表。
2. 该数据结构的一个应用是，确定无向图的连通分量。
3. 用链表表示不相交集合，每个集合有属性head和tail，分别指向单链表的头尾成员，链表成员有next和指向  
  该成员所在集合的指针。
4. 要合并两个集合，至少要把其中一个集合的每个成员的集合指针修改掉。执行n次make-set和n-1次union  
  操作，需要时间θ(n<sup>2</sup>)，摊还到每次操作是θ(n)。考虑union可以把小集合合并到大集合中，时间得以优化，  
  使得m个操作序列（含n个make-set）需要的时间为O(m+nlgn)。因为union操作都是小挂大集合上，第一次  
  执行后的结果集合至少有2成员，第二次至少4成员，……达到n成员只需lgn次对成员的集合指针的修改，故   
  总操作时间为O(n lgn)。

《算法导论》July 13 328-338
1. 不相交集合也可以用树表示，即**不相交集合森林**，每个结点有父指针，根节点的父是自己。它支持的操作：
   1. make-set(x)创建一棵只有一个结点的树。
   2. find-set(x)通过沿着父指针找到根。
   3. union(x, y)合并两个树，让一个树根的父指针指向另一个根。
2. 有了链表合并的启发，有**按秩合并**的策略，基本思想差不多是让节点少的树的根指向多的，每个结点维护  
  一个秩，表示该结点高度的一个上界。
3. 第二种启发式策略是**路径压缩**，上溯根时将下面结点都指向根。
4. 将两种策略结合就得到了渐进最优的不相交集合数据结构。时间复杂度O(m α(n))，m和n仍然分别表示  
  make-set和总操作数。 α(n)是一个增长很慢的函数，所以该方法复杂度接近线性。


《算法导论》July 14 341-343
1. 图的表示方法有两种，邻接链表和邻接矩阵法。结点的集合和边的集合分别用V和E表示，G=(V, E)。
2. 每个结点有一个邻接链表，表示其邻居，更适合稀疏的图，不存在的边不会占用空间。需要的空间O(V+E)。
3. 邻接矩阵更适合稠密图，占用空间O(V<sup>2</sup>)，每个结点映射到二维矩阵的行和列上，矩阵内单元值为1表示它  
  所在行列对应节点间有边。优点是可立即获知任意两点间是否有边，但不方便给边加属性（如权重）。  
  表示有向图时，将矩阵转置就意味着将边反向。
4. 若将基于邻接链表的的有向图反向，所需操作——把链表的头加到后继每个结点作为头的链表中。

《算法导论》July 15 344-346
1. 树的**广度优先搜索**，以源结点s开始，逐步扫描邻居结点。在扫描中，在概念上给节点涂上白、灰或黑色。  
  结点第一次发现被当成白色，结点的邻接结点都已被发现则涂黑色，否则涂灰。
    ```
    BFS(G, s)
      for u in (G.V-{s})  // 整个图中所有非s的节点涂白色
        u.color = WHITE
        u.d = ∞
        u.π = nil
    s.color = GREY
    s.d = 0
    s.π = nil
    Q = {}
    enqueue(Q, s)  // 
    while Q != {}
      u = dequeue(Q)
      for v in G.adj(u)  // 邻接结点，白色则涂灰放入队列
        if v.color == WHITE
          v.color = GREY
          v.d = u.d + 1
          v.π = u
          enqueue(Q, v)
      u.color = BLACK  // 扫描完邻接结点，变黑
    ```
   其复杂度是O(V+E)，第二项来源于各节点的邻接链表的扫描。
2. Q只保存灰色的结点，是完全扫描跟没扫描的结点的边界。
3. 定义s到结点v的**最短路径距离**为s到v的最少边数，BFS能找出给定源点s到所有可到达的结点的距离（d）。

《算法导论》July 16 347-384
1. **深度优先搜索**总是优先从新发现的结点出发往后搜索，与广度优先的前驱子图不同的是，深度优先搜索的  
  前驱子图是多棵互不相交的树，称为深度优先森林。

    ```
    dfs(G)
      for u ∈ G.V
        u.color = WHITE
        u.π = nil
      for u ∈ G.V
        if u.color == WHITE
        dfs-visit(G, u)
    dfs-visit(G, u)
      u.color = GREY
      for  u ∈ G:Adj[u]    // 沿着边uv搜索
        if v.color == WHITE
          v.π = u
          dfs-visit(G, v)
      u.color = BLACK      // 扫描完u的邻居后将u涂黑
    ```
   时间复杂度也为O(V+E)，因为每个节点的dfs-visit内的遍历邻居只会执行一次，这些遍历次数是E的常数倍。
2. 深度优先搜索中每个结点点的发现和完成时间构成括号化结构——若结点u先于v发现，则一定后于v完成。
3. 边的类型：
   1. 树边：深度优先森林G<sub>π</sub>中的边。
   2. 后向边：结点到其所在深度优先树的祖先结点的边（有向图的环包含后向边）。
   3. 前向边：跟后向边相反。
   4. 横向边：不在同一个深度优先树中的两个结点构成的边。  
   结合上面算法，当第一次探索(u,v)时可以根据v的颜色确定边的信息：v白色则边是树边，v灰色则边是  
   后向边，v黑色则可能是前向边或横向边。



《算法导论》July 18 355-361
1. **拓扑排序**：用于有向无环图（DAG）的排序算法。它可以对图中的结点进行排序，使得对于图中的每一条  
  有向边 (u, v)，结点 u 都排在结点 v 的前面。拓扑排序的实现过程如下：
   1. 创建一个队列，并将所有入度为 0 的结点加入队列。
   2. 从队列中取出一个结点，将其输出，并将其所有邻接结点的入度减 1。
   3. 如果邻接结点的入度变为 0，则将其加入队列。
   4. 重复步骤 2 和步骤 3，直到队列为空。
   5. 如果输出的结点数量等于图中的结点数量，则拓扑排序成功；否则，图中存在环，无法进行拓扑排序。  
     拓扑排序可以用于解决多个任务之间的依赖关系，例如编译器中的源代码编译顺序、任务调度等场景。  
   
2. **强联通分量**：若图中任意两个结点 u 和 v，存在从 u 到 v 和从 v 到 u 的路径，则称结点u和结点v是强连通。  
  而图的强连通分量是指图中的结点被划分为多个最大强连通子图的集合。  
   强连通分量的查找可以使用强连通分量算法。具体步骤如下：
   1. 对图进行一次深度优先搜索，将遍历的结点按照遍历结束的顺序依次压入栈中。
   2. 将图反向（将有向边的方向反转），再深度优先搜索，每次以从栈中取出的结点作为起始点搜索。
   3. 在第二次深度优先搜索中，每次遍历到一个结点，将它及其可达的所有结点加入到一个强连通分量中。
   4. 重复步骤2和步骤3，直到栈为空。最终，得到的强连通分量集合即为图的强连通分量。

《算法导论》July 19 362-373
1. 最小生成树（Minimum Spanning Tree）是指在一个连通无向图中，找到一棵包含所有顶点的树，且树上  
  所有边的权重之和最小。最小生成树应用范围很广，比如网络设计、电力传输、城市规划等。常用的算法  
  来求解最小生成树问题有Prim算法和Kruskal算法，它们都是贪心算法。
2. Prim算法：
    - 选择一个起始顶点，将其加入最小生成树中。
    - 从与最小生成树相邻的顶点中选择一个权重最小的边，将其加入最小生成树。
    - 重复上述步骤，直到最小生成树包含所有顶点。  
   选权重最小的边可用优先级队列。时间复杂度是O(ElgV)。  
3. Kruskal算法：
    - 将所有边按照权重从小到大进行排序。
    - 依次选择权重最小的边，如果该边的两个顶点不在同一个连通分量中，则将其加入最小生成树，并合并  
      两个连通分量。
    - 重复上述步骤，直到最小生成树包含所有顶点。  
    用秩合并和路径压缩算法，时间复杂度也为O(ElgV)。

《算法导论》July 21 374-380
1. **单源最短路径**问题：给定一个图G=(V,E)，找从给定源结点s到每个结点v的最短路径。它包含一些变体：  
  单目的地最短路径问题、单节点对最短路径问题、所有结点对最短路径问题、最短路径的最优子结构。
2. 定理：最短路径的子路径也是最短路径。可用反证法加剪切替换的思路证明。
3. 单源最短路径问题允许存在负权重的边，但不能包含负权重的环路，否则可以一直转圈减小代价，最短路径    
  就没有定义了。最终生成的结果是以s为根的树。
4. 为了计算和表示最短路径，每个结点可以持有一个前驱节点的指针π，属性d用来存最短路径的估计。  
  过程中利用**松弛**操作——将可以改善u、v路径长度的第三个结点加入其中（总权重变小，结点变多）。

《算法导论》July 23 381-387  
1. 有向无环图的**单源最短路径**问题：在一个有向无环图中，找到从给定起点到所有其他顶点的最短路径。  
  这个问题在路由算法、任务调度等多个场景都有运用。 常用的算法有Dijkstra算法和Bellman-Ford算法。
2. Dijkstra算法：
    - 初始化起点到所有其他顶点的距离为无穷大，起点到自身的距离为0。
    - 选择一个未访问的顶点，计算起点经过该顶点到其他顶点的距离，更新距离数组。
    - 重复上述步骤，直到所有顶点都被访问过。  
      时间复杂度是O((V + E) log V)，其中logV是每次用优先级队列选节点操作的复杂度。
3. Bellman-Ford算法：
    - 初始化起点到所有其他顶点的距离为无穷大，起点到自身的距离为0。
    - 依次对所有边进行松弛操作，即通过该边缩短起点到其他顶点的距离。
    - 重复上述步骤，直到没有边可以进行松弛操作或者存在负权回路。  
  时间复杂度O(V * E)，每个边需要执行V-1次松弛操作。

《算法导论》July 24 387-391
1. 线性规划：给定m*n的矩阵A、一个m维向量b和n维向量c，去找到一个n维向量x，使得在Ax<=b的条件约束  
  下，函数Σx<sub>i</sub>c<sub>i</sub>取得最值。单源最短路径问题就是线性规划的例子。
2. 在一个查分约束系统重，A的每行包括一个1和一个-1，其他项均为0. Ax<=b可看作m个差额限制条件，其中  
  每个约束条件是形如x<sub>j</sub> - x<sub>i</sub> <= b<sub>k</sub>的简单线性不等式。
3. 从图论的角度看查分约束系统，矩阵A则可以被看作n个结点和m条边构成的图的邻接矩阵的转置。约束条件  
   x<sub>j</sub> - x<sub>i</sub> <= b<sub>k</sub>可以表示边(v<sub>i</sub>, v<sub>j</sub>)的权重为b<sub>k</sub>，故可以通过求解约束图来找到一个差分约束系统的解。
4. Bellman-Ford算法就是其实现，基于定理——差分系统Ax<=b对应于图G=(V, E)，则它有一个解是  
  x = (δ(v<sub>0</sub>, v<sub>1</sub>), δ(v<sub>0</sub>, v<sub>2</sub>), ..., δ(v<sub>0</sub>, v<sub>n</sub>))。

《算法导论》July 26 391-398
1. 单源最短路径的证明依赖三角不等式、上界性质、非路径性质、收敛性质、路径松弛性质和  
  前驱子图性质，下面是它们的内容。
2. 三角不等式性质：带权重的有向图的权重函数由w:E->R给出，源结点为s，则任意属于E的边(u,v)满足  
   δ(s,v) <= δ(s,u) + w(u, w).  
   δ是最短路径函数，反证法直接得证。
3. 上界性质：图G初始化。那么对于所有节点v∈v, 有v.d>=δ(s, v)，且该不等式在对G的边进行任何次序的松弛  
  过程中保持成立，且一旦v.d取得其下界δ(s, v)后，将不再发生变化。  
  这个结论也很直观，用归纳法证：  
  基础：初始化后显然成立。  
  归纳：v.d >= u.d + w(u, v) >= δ(s, u) + w(u, v) >= δ(s, v)，最后一步利用了三角不等式。  
  循环不变式得到维持。
4. 非路径性质：结点s到v间不存在路径，则初始化后有v.d = δ(s, v) = ∞，等式维持到G的所有松弛操作结束。  
  证明：根据上界性质，∞ = δ(s, v) <= v.d，得证。
5. （引理）收敛性质：设s为源结点，s~>u->v为图G最短路径，然后进行初始化和一系列松弛操作。若在对  
  边(u, v)进行松弛操作之前的任意时刻有u.d= δ(s, u)，则在松弛后所有时刻有v.d =  δ(s, v)。
6. （引理）路径松弛性质：s到结点v<sub>k</sub>的任意一条最短路径p=<v<sub>0</sub>, v<sub>1</sub>, ...,<sub>k</sub>>，松弛操作次序为(v<sub>0</sub>, v<sub>1</sub>)，(v<sub>1</sub>,   
  v<sub>2</sub>)，…, (v<sub>k-1</sub>, v<sub>k</sub>)，则所有操作后有v<sub>k</sub>.d = δ(s, v<sub>k</sub>)，并且在此后该等式一直保持成立。


《算法导论》July 27 399-404
1. 探究完单源最短路径后，下面看下所有结点对的最短路径问题。目标是找到任意结点对间的最短路径权重  
  以及前驱结点矩阵II=(π<sub>ij</sub>)，其中π<sub>ij</sub>是i到j最短路径上的j的前驱结点。
2. 该问题可以用动态规划算法解，步骤：
   1. 根据最短路径的子路径也是最短路径的性质，最短路径结构为 δ(i,j) = δ(i,k) + w(k, j)，其中w为  
    邻接矩阵表示的k到j的路径权重。
   2. 用l(i,j,m)表示i到j的至多包含m条边的任意路径中的最小权重。当m=0时，若i=j则l=0，否则l=无穷大。  
     m>=1时，l(i,j,m)为l(i,j,m-1)的最小值和i到j最多有m条边组成的任意路径的最小者。即递归定义  
      l(i,j,m) = min (l(i,j,m-1), min<sub>1~n</sub>{l(i,k,m-1)+w<sub>kj</sub>}) = min<sub>1~n</sub>{l(i,k,m-1) + w<sub>kj</sub>}
   3. 自底向上计算最短路径权重：在给定矩阵W=(w<sub>ij</sub>)和L(i,j,m-q)的情况下计算L(i,j,m)
   ```
   extend-shorteset-path(L, W)
     n = L.rows
     let L' = l'(i,j) be a new n*n matrix
     for i=1 to n
       for j=1 to n
         l'(i,j) = ∞
           for k=1 to n
             l'(i,j)=mian(l'(i,j), l'(j,k) + w(k,j))
   
     return L'
   ```
   该算法可看做矩阵叉乘A×B=C，其中A为L(m-1)，C为L(m)，B为W。考虑我们只对L(n-1)（边数上界最小的  
   最短路径）感兴趣，L(m)=L(n-1)。可以用lg(n-1)个矩阵乘积来算。再考虑每个矩阵乘积用重复平方技术计算  
   可以得到时间复杂度为O(n<sup>3</sup>lgn)。

《算法导论》July 28 405-407
1. Floyd-Warshall算法用的是另一种动态规划算法，其核心是根据中间结点跟i~j路径的关系分解问题。其中  
  中间结点指除两端外的任意点。假定G的所有结点为V = {1, 2, ..., n}，考虑其中一个子集{1, 2, ..., k}，其中  
  k<n，对于任意结点对i, j∈ V，考虑i到j的所有中间结点都取自该子集，设路径p是权重最小的路径。
   1. 如果结点k不是p上的中间结点，则去掉它后的子集{1, 2, ..., k-1}仍包含所有中间结点。
   2. 否则，p可以被k分为p1和p2这两个最短路径，且它们各自的中间结点都在{1, 2, ..., k-1}。
2. 用D(n)表示最短路径权重矩阵，自底向上最短路径权重的算法：
    ```
    floyd-warshall(W)
      n = W.rows
      D(0) = W
      for k=1 to n
        let D(k) (d(i,j,k)) be a new n×n matrix
        for i=1 to n
          for j=1 to n
            d(i,j,k) = min(d(i,j,k-1), d(i,k,k-1) + dd(k,j,k-1))
      return D(n) 
    ```
    算法时间复杂度为O(n<sup>3</sup>)。另外，有两种构建前驱矩阵的方法，一是有D得到，二是在计算D(k)的同时得到II  
  矩阵序列。

《算法导论》July 29 407-409
1. 有向图的传递闭包，指对于有向图中的每对结点，如果存在一条从结点i到结点j的路径，那么在传递闭包中  
  就会存在一条从结点i到结点j的直接或间接路径。即传递闭包包含了图中所有可能的路径。
2. 传递闭包可以通过使用传递闭包算法来计算。这个算法使用矩阵表示图的连接关系，并通过矩阵运算来确定  
  传递闭包。具体而言，算法通过迭代计算矩阵的乘积，直到达到传递闭包的最终状态。算法可以应用于  
  网络分析、图数据库和编译器优化等。
3. 传递闭包算法伪代码：
    ```
    transitive-closure(G):
      n = |G.V|
      closure = createEmptyMatrix(n, n)
       // 初始化闭包矩阵
      for i from 0 to n-1:
        for j from 0 to n-1:
          closure[i][j] = G.edgeExists(i, j)
       // 使用Floyd-Warshall算法计算传递闭包
      for k from 0 to n-1:
        for i from 0 to n-1:
          for j from 0 to n-1:
            closure[i][j] = closure[i][j] OR (closure[i][k] AND closure[k][j])
       return closure
    ```


《算法导论》July 30 409-413  
1. Johnson算法也是用于求解有向图中所有节点对之间最短路径的算法。它引入了一个虚拟节点，并对图进行  
  一次变换，然后利用Bellman-Ford算法和Dijkstra算法来计算最短路径。
2. 以下是Johnson算法的步骤：  
   1. 添加一个新的虚拟节点到原始图中，并将其与所有其他节点之间添加一条权重为0的边。
   2. 使用Bellman-Ford算法计算从虚拟节点到其他所有节点的最短路径。如果存在负权重环路，则算法会  
     检测到并停止执行。
   3. 根据Bellman-Ford算法的结果，重新计算图中每条边的权重。对于原始图中的每条边 (u, v)，将其权重  
     更新为原始权重+从虚拟节点到u的最短路径长度-从虚拟节点到v的最短路径长度。
   4. 对于每对顶点u和v，使用Dijkstra算法计算从u到v的最短路径。这里的权重是经上一步计算后的新权重。  
     最终得到的结果是一个最短路径矩阵，矩阵的每个元素表示从一个顶点到另一个顶点的最短路径长度。  
3. 时间复杂度为O(V^2 log V + VE)。处理稀疏图时效果较好。


《算法导论》July 31 414-417
1. 流网络用于描述一种有向图，其每条边都带有一个非负的容量（capacity），表示该边能够承载的最大  
  流量。流网络通常用于建模流量、网络、管道等问题。  
2. 在流网络中，通常有两个特殊的节点，称为源节点（source）和汇节点（sink）。分别是流量的起点和  
  终点。除了源节点和汇节点，流网络中可能还存在其他的中间节点。  
3. 流网络中的流量（flow）是指从源节点到汇节点的路径上通过的流量量。每条边上的流量不能超过其容量，  
  且满足守恒性：任何一个节点（除了源节点和汇节点）的流入流量等于流出流量。  
4. 在流网络中，常用的问题是求解最大流（maximum flow）和最小割（minimum cut）。
   1. 最大流问题是找到从源节点到汇节点的路径上能够通过的最大流量。
   2. 最小割问题是将流网络分为两部分，使得源节点和汇节点在不同的部分，并且割的容量最小。


《算法导论》Aug 2 418-424
1. Ford-Fulkerson方法是一种经典的用于解决最大流问题的方法。它基于增广路径的思想，通过不断寻找  
  增广路径来逐步增加流量，直到无法找到增广路径为止。基本步骤：
   1. 初始化流网络：给定一个流网络，初始化所有边的流量为0。
   2. 寻找增广路径：在流网络中寻找一条从源节点到汇节点的增广路径。可以使用DFS或BFS等算法来  
   查找增广路径。
   3. 计算增广路径上的最小剩余容量：在找到增广路径后，计算该路径上各边的最小剩余容量，即该路径  
   上容量最小的边的容量。
   4. 更新流量：将步骤3中计算得到的最小剩余容量，加到增广路径上的每条边的流量中。
   5. 重复步骤2-4：重复执行步骤2至步骤4，直到无法找到增广路径，流网络中的流量已达到最大。  
2. Ford-Fulkerson方法的关键在于选择合适的增广路径。通常，可以使用DFS或BFS来搜索增广路径，但  
  不同的选择可能会导致不同的算法性能。为了提高效率，可以使用一些优化技巧，如最短增广路径优先  
  或最大容量优等。
3. Ford-Fulkerson方法本身并没有明确的终止条件。为了确保算法的终止和正确性，可以使用Edmonds-Karp  
  算法，它基于BFS来寻找增广路径，并保证了算法的多项式时间复杂度。

《算法导论》Aug 4 425-428
1. 最大流问题用到Ford-Fulkerson算法，使用广度优先搜索来选择增广路径，从而提高了效率。步骤如下：
   1. 初始化网络的流量为0。
   2. 使用BFS从源节点开始，找到一条增广路径（在网络中每条边的剩余容量大于0的一条路径）
   3. 如果找到增广路径，则通过该路径增加流量，并更新每条边的剩余容量和反向边的容量。
   4. 重复步骤i和iii，直到无法找到增广路径为止。
   5. 返回最终的最大流量。
2. Edmonds-Karp算法的时间复杂度为O(V * E^2)，其中V是节点数，E是边数。相比于Ford-Fulkerson算法，  
  其优势在于使用BFS寻找增广路径，可以保证每次寻找的路径长度最短，效率得以提高。

《算法导论》Aug 5 428-450
1. 最大二分匹配问题，也被称为二分图最大匹配问题，其目标是在一个二分图中找到最大的匹配数量。其中，  
   匹配指的是将图中的节点按照特定规则进行配对。每个节点只能与另一个节点进行匹配，有一条边相连。
2. 在使用Ford-Fulkerson方法解决最大二分匹配问题时，首先需要将二分图转化为一个有向图。然后通过不断  
  寻找增广路径，将流量从一个节点传递到另一个节点，直到无法找到增广路径为止。最终，最大匹配的数量  
  就是流量通过的总量。
3. 二分图G中的一个最大匹配M的基数等于其对应的流网络G'中某一最大流f的值。
4. 由于二分图中的任意匹配的基数最大值为O(V)，因此可以在O(VE)时间内找到最大匹配。

《算法导论》Aug 6 451-460
1. 动态多线程是软件层协调、调度和管理并行计算资源（取代静态线程）的平台。其优点有：
   1. 是串行编程的简单扩展，通过在伪代码中加入三个并发关键词parallel、spawn和sync来描述多线程。
   2. 从理论上提供了一个基于工作量和持续时间概念的方式来量化并行性。
2. 以斐波那契数为例，串行算法：
    ```
    fib(n):
      if n<= 1
        return n
      x = fib(n-1)
      y = fib(n-2)
      return x + y
    ```
   动态多线程算法：
    ```
    p_fib(n):
      if n<= 1
        return n
      x = spawn p_fib(n-1)
      y = p_fib(n-2)
      sync
      return x + y
    ```
    后者只比前者多了两个关键字，其中spawn派生出子进程，跟父并行执行，子进程计算x，父进程计算y。
3. 从图的角度看，上述算法产生一个调度树，或者**计算有向无环图**。每次spawn生成派生边，另递归调用  
  生成调用边。

《算法导论》Aug 9 461-468  
多线程矩阵乘法：
1. 初始化矩阵A和矩阵B，定义结果矩阵C。
2. 检查矩阵A和矩阵B的维度是否满足矩阵乘法的要求。
3. 创建一个多线程任务队列，用于存储每个线程要执行的任务。
4. 根据线程数量，将矩阵C划分为相应数量的子矩阵。
5. 遍历每个子矩阵，创建一个线程任务，将任务添加到任务队列中。
6. 启动线程池，从任务队列中获取任务并执行。
7. 每个线程任务接收一个子矩阵的索引，根据索引计算子矩阵乘法，并将结果存储在对应的子矩阵中。
8. 所有线程任务执行完毕后，等待线程池中的所有线程完成。
9. 输出结果矩阵C。 


《算法导论》Aug 10 468-472
1. 多线程执行归并排序——将排序任务划分为多个子任务，然后并行地对这些子任务进行排序和合并。
    1. 划分任务：多线程归并排序首先将待排序的数组划分为两个子数组，每个分配给一个线程进行排序。
    2. 并行排序：每个线程独立地对分配到的子数组进行归并排序。这些线程可以同时执行，即并行排序。
    3. 合并结果：排序完成后，将每个线程排序得到的有序子数组进行合并。
    4. 递归调用：如果合并得到的有序子数组仍然很大，可以递归地划分为更小的子数组，再执行并行排序  
   和合并操作。
    5. 最终合并：通过递归调用，最终将所有子数组合并为一个完整的有序数组。  
    多线程归并排序的优势：可以利用多核处理器的并行计算能力，加快排序速度。但带来了线程间的  
   同步和数据共享的开销。
伪代码：
   ```
   function parallelMergeSort(arr):
       if length of arr <= 1:
           return arr
       left,right = split(arr)
       spawn sort(left)
       sort(right)
       sync
       return merge(left, right)
   ```

《算法导论》Aug 12 481-486  
假设有一个n×n的矩阵A，我们的目标是将其分解为一个下三角矩阵L和一个上三角矩阵U的乘积。
1. 初始化：给定一个n×n的矩阵A作为输入。
2. LU分解的步骤：  
   a. 创建一个n×n的零矩阵L和U，其中L是下三角矩阵，U是上三角矩阵。  
   b. 对于矩阵A的第一行，将其复制到矩阵U的第一行。  
   c. 对于矩阵A的第一列，将其除以矩阵U的第一行的第一个元素，并将结果存储在矩阵L的第一列中。  
   d. 对于剩余的行和列，使用以下公式计算矩阵L和U的元素：  
    - 对于矩阵L的第i行和第j列的元素（其中i > j），使用以下公式：  
      L(i, j) = (A(i, j) - Σ(L(i, k) * U(k, j))) / U(j, j)，其中k的取值范围是1到j-1。
    - 对于矩阵U的第i行和第j列的元素（其中i ≤ j），使用以下公式：  
      U(i, j) = A(i, j) - Σ(L(i, k) * U(k, j))，其中k的取值范围是1到i-1。
   
   e. LU分解完成后，我们得到了下三角矩阵L和上三角矩阵U，它们的乘积等于原始矩阵A。  
3. 结果：得到了矩阵A的LU分解，其中下三角矩阵L包含了A的行变换信息，上三角矩阵U包含了A的  
  列变换信息。

《算法导论》Aug 17 495-499  
1. 一般线性规划问题对应一组线性不等式，它通常具有以下形式：
   1. 最大化（或最小化）目标函数：Z = c<sub>1</sub>x<sub>1</sub> + c<sub>2</sub>x<sub>2</sub> + ... + c<sub>n</sub>x<sub>n</sub>
   2. 同时满足一组线性不等式约束条件：  
    a<sub>11</sub>x<sub>1</sub> + a<sub>12</sub>x<sub>2</sub> + ... + a<sub>1n</sub>x<sub>n</sub> ≤ b<sub>1</sub>  
    a<sub>21</sub>x<sub>1</sub> + a<sub>22</sub>x<sub>2</sub> + ... + a<sub>2n</sub>x<sub>n</sub> ≤ b<sub>2</sub>  
    ...  
    a<sub>m1</sub>x<sub>1</sub> + a<sub>m2</sub>x<sub>2</sub> + ... + a<sub>mn</sub>x<sub>n</sub> ≤ b<sub>m</sub>  
   其中，x1, x2, ..., xn 是决策变量，表示需要确定的值。c<sub>i</sub> 是目标函数中的系数，表示决策变量的权重。
  a<sub>ij</sub> 是约束条件中的系数，表示决策变量在约束条件中的影响。b<sub>i</sub> 是约束条件中的常数项。
2. 线性规划问题的目标是找到一组满足约束条件的决策变量值，使得目标函数达到最值。
3. 线性规划算法：
   1. 单纯形算法，基于线性规划问题的凸性和可行解空间的顶点特性，逐步移动逼近最优解。
   2. 椭球算法：效率低。
   3. 内点法：它通过在可行解空间内部搜索最优解，而不是在边界上移动。内点法通常比单纯形法更高效，  
    适用于大规模问题。